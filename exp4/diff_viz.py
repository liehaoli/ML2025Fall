import os
import torch
import torchvision.utils as vutils
import matplotlib.pyplot as plt
import numpy as np
from tqdm.notebook import tqdm
from utils.unet import SimpleUNet

# --- 1. å¯è§†åŒ–é…ç½®å‚æ•° ---
class VizConfig:
    # è·¯å¾„é…ç½®
    model_path = './best_diffusion_model.pth'
    output_dir = './output_diff_steps'  # ğŸ†• å¯è§†åŒ–ç»“æœä¿å­˜è·¯å¾„
    
    # ç”Ÿæˆå‚æ•°
    num_images = 64              # ä»…éœ€ç”Ÿæˆ 64 å¼ 
    
    # æ‰©æ•£æ¨¡å‹å‚æ•° (å¿…é¡»ä¸è®­ç»ƒä¸€è‡´)
    num_timesteps = 1000
    beta_start = 0.0001
    beta_end = 0.02
    
    # ç³»ç»Ÿå‚æ•°
    seed = 42
    no_cuda = False

args = VizConfig()

def visualize_diffusion_process():
    # 1. ç¯å¢ƒè®¾ç½®
    global device
    if 'device' not in globals():
        device = torch.device("cuda" if not args.no_cuda and torch.cuda.is_available() else "cpu")
    print(f"Running visualization on: {device}")
    
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
        
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
        print(f"Created output directory: {args.output_dir}")

    # 2. å‡†å¤‡å‚æ•°
    betas = torch.linspace(args.beta_start, args.beta_end, args.num_timesteps, device=device)
    alphas = 1.0 - betas
    alphas_cumprod = torch.cumprod(alphas, dim=0)
    sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)
    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)

    # 3. åŠ è½½æ¨¡å‹
    try:
        model = SimpleUNet().to(device)
        if os.path.isfile(args.model_path):
            print(f"Loading model from {args.model_path}...")
            checkpoint = torch.load(args.model_path, map_location=device, weights_only=False)
            model.load_state_dict(checkpoint['model'])
            print("Model loaded successfully!")
        else:
            print(f"Model not found at {args.model_path}")
            return
    except Exception as e:
        print(f"Error: {e}")
        return

    model.eval()

    # --- 4. å¸¦å¯è§†åŒ–ä¿å­˜çš„é‡‡æ ·å‡½æ•° ---
    print(f"Starting visualization process (Total Steps: {args.num_timesteps})...")
    
    with torch.no_grad():
        # 1. ä»çº¯é«˜æ–¯å™ªå£°å¼€å§‹ x_T (Step 1000)
        x = torch.randn(args.num_images, 3, 32, 32, device=device)
        
        # ä¿å­˜åˆå§‹å™ªå£°çŠ¶æ€ (Step 1000)
        # ä¸¥æ ¼æ¥è¯´è¿™è¿˜æ²¡å¼€å§‹å»å™ªï¼Œä½†ä¸ºäº†å®Œæ•´æ€§å¯ä»¥å­˜ä¸€ä¸‹ï¼Œæˆ–è€…æŒ‰ä½ çš„è¦æ±‚åªå­˜æ•´ç™¾æ­¥
        # vutils.save_image(x, f"{args.output_dir}/diffusion_step_noise.png", normalize=True, nrow=8)
        
        # 2. å€’åºå»å™ª T-1 -> 0
        for i in tqdm(reversed(range(0, args.num_timesteps)), desc="Denoising Progress", total=args.num_timesteps):
            t = torch.full((args.num_images,), i, device=device, dtype=torch.long)
            
            # é¢„æµ‹å™ªå£°
            noise_pred = model(x, t)
            
            # æå–ç³»æ•°
            beta_t = betas[i]
            sqrt_one_minus_alpha_cumprod_t = sqrt_one_minus_alphas_cumprod[i]
            sqrt_recip_alpha_t = sqrt_recip_alphas[i]
            
            # è®¡ç®—å‡å€¼ (x_{t-1})
            mean = sqrt_recip_alpha_t * (x - (beta_t / sqrt_one_minus_alpha_cumprod_t) * noise_pred)
            
            if i > 0:
                noise = torch.randn_like(x)
                sigma = torch.sqrt(beta_t)
                x = mean + sigma * noise
            else:
                x = mean
            
            # --- ğŸ†• å¯è§†åŒ–ä¿å­˜é€»è¾‘ ---
            # æ¯ 100 æ­¥ä¿å­˜ä¸€æ¬¡ï¼Œæˆ–è€…æœ€åä¸€æ­¥ä¿å­˜
            if i % 100 == 0 or i == 0:
                save_path = os.path.join(args.output_dir, f'diffusion_step_{i}.png')
                vutils.save_image(x, save_path, normalize=True, nrow=8)
                # ä»…æ‰“å°å…³é”®èŠ‚ç‚¹æ—¥å¿—ï¼Œé¿å…åˆ·å±
                if i % 200 == 0 or i == 0:
                    print(f"   ğŸ“¸ Snapshot saved: Step {i}")

    print(f"Visualization completed! Check images in {args.output_dir}")
    
    # --- 5. å±•ç¤ºç»“æœ (å±•ç¤ºé¦–å°¾) ---
    steps_to_show = [500, 300, 100, 0]
    
    print(f"\n Displaying steps: {steps_to_show}")
    
    # åˆ›å»º 2è¡Œ x 3åˆ— çš„ç”»å¸ƒ
    fig, axs = plt.subplots(2, 2, figsize=(15, 10))
    axs = axs.flatten()
    
    for idx, step in enumerate(steps_to_show):
        img_path = os.path.join(args.output_dir, f'diffusion_step_{step}.png')
        if os.path.exists(img_path):
            img = plt.imread(img_path)
            axs[idx].imshow(img)
            axs[idx].set_title(f"Step {step}")
            axs[idx].axis("off")
        else:
            print(f"âš ï¸ Warning: Image for step {step} not found.")
            axs[idx].axis("off")
            
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    visualize_diffusion_process()
